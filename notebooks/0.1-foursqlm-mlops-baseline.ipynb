{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-04T05:20:55.705883Z","iopub.status.busy":"2022-05-04T05:20:55.705293Z","iopub.status.idle":"2022-05-04T05:20:55.710929Z","shell.execute_reply":"2022-05-04T05:20:55.709947Z","shell.execute_reply.started":"2022-05-04T05:20:55.705844Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Префиксный код (импорты, вспомогательные функции/классы/гиперпараметры)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:40.238733Z","iopub.status.busy":"2022-05-02T16:32:40.238475Z","iopub.status.idle":"2022-05-02T16:32:41.539092Z","shell.execute_reply":"2022-05-02T16:32:41.538405Z","shell.execute_reply.started":"2022-05-02T16:32:40.238693Z"},"trusted":true},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import haversine as hs\n","import random\n","\n","# Imports from\n","from haversine import Unit\n","from sklearn.model_selection import GroupKFold\n","from fuzzywuzzy import fuzz\n","from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.541334Z","iopub.status.busy":"2022-05-02T16:32:41.540807Z","iopub.status.idle":"2022-05-02T16:32:41.546388Z","shell.execute_reply":"2022-05-02T16:32:41.54508Z","shell.execute_reply.started":"2022-05-02T16:32:41.541285Z"},"trusted":true},"outputs":[],"source":["# Fix all random seeds\n","random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.548257Z","iopub.status.busy":"2022-05-02T16:32:41.54785Z","iopub.status.idle":"2022-05-02T16:32:41.581993Z","shell.execute_reply":"2022-05-02T16:32:41.581393Z","shell.execute_reply.started":"2022-05-02T16:32:41.548221Z"},"trusted":true},"outputs":[],"source":["# Custom Func's/Class's\n","\n","def jaccard(list_a: list, list_b: list) -> float:\n","    a = set(list_a)\n","    b = set(list_b)\n","    return len(a & b) / len(a | b)\n","\n","\n","def jaccard_score(df_sub_a: pd.DataFrame, df_sub_b: pd.DataFrame) -> float:\n","    \n","    assert len(df_sub_a) == len(df_sub_b)\n","    df = pd.merge(df_sub_a, df_sub_b, on='id', suffixes=['_a','_b'])\n","    return (df.apply(lambda x: jaccard(x.matches_a.split(), x.matches_b.split()),\n","                     axis=1)).mean()\n","\n","\n","def get_submission_predict(df_original: pd.DataFrame, \n","                           df_pairs: pd.DataFrame, \n","                           labels: np.array,\n","                           pairs_drop_orders_dublicates = False) -> pd.DataFrame:\n","    \n","    # task: понял поздно, уже после написания, но есть проблема, нужно ее решить,\n","    #       переписать в единственном варианте, либо добавить возможность возвращать\n","    #       оба варианта, в зависимости от перданного параметра (и проверить на\n","    #       реальных сабмитах, какой вариант дает больший скор);\n","    #       проблема: если модель предсказала, что (id1, id2) и (id2, id3) дублиаты,\n","    #       а (id1, id3) нет, то в текущей реализации в сабмит добавятся строки \n","    #       (id1, [id1, id2]), (id2, [id2, id1, id3], (id3, [id3, id2]), хотя на\n","    #       самом деле они либо все дубликаты, либо где то предсказание ошибочно.\n","    \n","    # task: возможно медленно работает, протестировать с прошлой версией во времени\n","    \n","    # формируем датасет из пар, для которых match/label == 1\n","    df_pairs = df_pairs[['id_1', 'id_2']]\n","    df_pairs['match'] = labels\n","    df_pairs = df_pairs[df_pairs.match == 1][['id_1','id_2']]\n","    \n","    # если мы оставляли пары только в одном направлении (id_1, id_2),\n","    # то возвращаем что бы они были в обоих (id_1, id_2) и (id_2, id_1)\n","    if pairs_drop_orders_dublicates:\n","        df_pairs = (pd.concat([df_pairs, \n","                               df_pairs.rename(columns={'id_1': 'id_2', \n","                                                        'id_2': 'id_1'})])\n","                    .drop_duplicates()) #drop_duplicates не обязателен\n","    \n","    # добавляем сапоставление  id  самому себе, т.к. этого требует выходной\n","    # формат\n","    pairs_one_to_one = pd.DataFrame({'id_1': df_pairs.id_1.unique()})\n","    pairs_one_to_one['id_2'] = pairs_one_to_one.id_1\n","    df_pairs = pd.concat([pairs_one_to_one, df_pairs])\n","    \n","    # переводим в формат id, matches, где в matches через пробел перечислены все \n","    # найденные дубликаты (в том числе сам id попадет в matches)\n","    df_pairs = (df_pairs.groupby('id_1').id_2.agg(' '.join).to_frame().reset_index()\n","                .rename(columns={'id_1': 'id', 'id_2': 'matches'}))\n","    \n","    # в df_pairs остались только id, для которых найдены дубликаты, мерджим со \n","    # всеми id из исходного датасета и добавляем в matchs id самого себя, для \n","    # тех id, которые не попали в df_pairs (после merge у них matches == NaN)\n","    df_submission = pd.merge(df_original['id'], df_pairs, on='id', how='left')\n","    df_submission['matches'] = df_submission.matches.fillna(df_submission.id)\n","    \n","    assert len(df_submission) == len(df_original)\n","    \n","    return df_submission\n","\n","\n","def get_submission_true(df_original: pd.DataFrame) -> pd.DataFrame:\n","    df_original = df_original[['id', 'point_of_interest']]\n","    df_poi_matches = (df_original.groupby('point_of_interest').id.agg(' '.join)\n","                      .to_frame().reset_index().rename(columns={'id': 'matches'}))\n","    return pd.merge(df_original, df_poi_matches, \n","                    on='point_of_interest', how='left')[['id','matches']]\n","\n","def get_pairs_metrics(df_original: pd.DataFrame, \n","                      df_pairs: pd.DataFrame, \n","                      labels: np.array,\n","                      pairs_drop_orders_dublicates = False) -> dict:\n","    metrics = {}\n","    submission_true = get_submission_true(df_original)\n","    submission_pairs_max_true = get_submission_predict(df_original, \n","                                                       df_pairs, \n","                                                       labels, \n","                                                       pairs_drop_orders_dublicates)\n","    metrics['Jaccard (max)'] = jaccard_score(submission_true, submission_pairs_max_true)\n","    \n","    return metrics\n","\n","def generate_pairs_df(df_dataset: pd.DataFrame, drop_order_dub=False, get_metrics = False, real_test = False) -> (pd.DataFrame, dict):\n","    # Отбираем только колонки, которые планируем использовать в дальнейшем\n","    # task: перенести в параметры/гиперпараметры?\n","    \n","    FIRST_COLUMN_SELECTION = ['id', 'name', 'latitude', 'longitude', 'country', 'city', 'categories', 'point_of_interest']\n","    \n","    # в реальном тесте отсутсвует 'point_of_interest', удаляем из наших колонок\n","    if real_test:\n","        FIRST_COLUMN_SELECTION.remove('point_of_interest')\n","    \n","    \n","    df_dataset = df_dataset[FIRST_COLUMN_SELECTION]\n","\n","    # task: Изучить возможности sklearn для подобных целей, скорей всего это будет наилучший вариант (примерные ключевые слова:\n","    # sklearn neighbors by coordinate)\n","    FIRST_COORD_ROUND = 3 # сотые ~= 1 км, тысячные ~= 0.1 км\n","\n","    # task: Если использовать подобный подход, нужно обязательно производить с перекрытием (придумать как)\n","    # task: Устранить SettingWithCopyWarning\n","\n","    # Первоначальный вариант (в 'lat_lon_round' мы получим строкивые представления округленных координат):\n","    # df_dataset.loc['lat_lon_group'] = (df_dataset.latitude.map(lambda x: str(round(x,FIRST_COORD_ROUND))) + '_' + \n","    #                                    df_dataset.longitude.map(lambda x: str(round(x,FIRST_COORD_ROUND))))\n","    # Альтернативный вариант (результат аналогичный, за исключенем того, что в 'lat_lon_round' мы получим номера групп):\n","    df_dataset['lat_lon_group'] = df_dataset.groupby([df_dataset.latitude.round(FIRST_COORD_ROUND), \n","                                                      df_dataset.longitude.round(FIRST_COORD_ROUND)]).ngroup()\n","    \n","    ## Формирование пар-кандидитов\n","    columns_to_pairs = ['lat_lon_group'] #колонки для совоставления в пары\n","    df_pairs = pd.merge(df_dataset, df_dataset, on=columns_to_pairs, suffixes=['_1','_2'])\n","\n","    # Оставляем пары только в одном направлении (id_1, id_2) или в обоих (id_1, id_2) и (id_2, id_1)\n","    if drop_order_dub:\n","        df_pairs = df_pairs[df_pairs.id_1 < df_pairs.id_2]\n","    else: #удаляем только полные дубликаты (id_1, id_1)\n","        df_pairs = df_pairs[df_pairs.id_1 != df_pairs.id_2]\n","    \n","    \n","    #Generate metrics for current candidates\n","    metrics = {}\n","    if get_metrics and not real_test:\n","        labels = np.array(get_match_label(df_pairs))\n","        metrics = get_pairs_metrics(df_dataset, \n","                                    df_pairs, \n","                                    labels,\n","                                    drop_order_dub)\n","\n","    return df_pairs, metrics\n","\n","def add_feauture_geo_distance(df_pairs: pd.DataFrame, normalize=False, prefix='ftr_') -> pd.DataFrame:\n","    # Считаем расстояние в км между точками\n","    # task: Возможно нет смысла считать точно через haversine (с учетом шарообразности земли), \n","    # можно считать более грубо, но быстрее\n","    \n","    df_pairs[f'{prefix}geo_distance'] = df_pairs.apply(lambda x: hs.haversine((x.latitude_1,x.longitude_1), \n","                                                 (x.latitude_2,x.longitude_2), \n","                                                 unit=Unit.KILOMETERS), axis=1)\n","    return df_pairs\n","\n","\n","def add_feauture_levenshtein_distance(df_pairs: pd.DataFrame, normalize=False, prefix='ftr_') -> pd.DataFrame:\n","    # Levenshtein distance of names\n","    df_pairs[f'{prefix}name_levenshtein'] = df_pairs.apply(lambda x: fuzz.token_set_ratio(x.name_1,\n","                                                                                          x.name_2), axis=1)\n","    return df_pairs\n","\n","\n","def run_futures_pipeline(dataset: pd.DataFrame, futures_pipeline: list, prefix='ftr_') -> pd.DataFrame:\n","    for step in futures_pipeline:\n","        dataset = step['func'](dataset, prefix=prefix, **step['params'])\n","    future_columns = [col for col in dataset.columns if col.startswith(prefix)]\n","    return dataset.reset_index(drop=True), future_columns\n","\n","\n","def get_match_label(dataset: pd.DataFrame) -> pd.Series: # 1 = match, 0 = not match\n","    #Наша целевая переменная (label/target)\n","    return (dataset['point_of_interest_1'] == dataset['point_of_interest_2']).astype(int)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.583753Z","iopub.status.busy":"2022-05-02T16:32:41.583513Z","iopub.status.idle":"2022-05-02T16:32:41.601279Z","shell.execute_reply":"2022-05-02T16:32:41.600404Z","shell.execute_reply.started":"2022-05-02T16:32:41.583726Z"},"trusted":true},"outputs":[],"source":["## Hyperparameters\n","\n","# Задаем пути до директории с train/test.csv (в записимости от варианта запуска ноутбука)\n","if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n","    PATH_TO_RAW_DATA = '/kaggle/input/foursquare-location-matching'\n","else:\n","    #./data/raw\n","    PATH_TO_RAW_DATA = os.path.join('.',\"data\", \"raw\")\n","    \n","# Количество частей, на которые будет разбит train датасет, \n","# одна часть уйдет на split_test, остальные на split_train\n","# 3 = 1/2 (или 33%/88%), 4 = 1/3 (или 25%/75) и т.д.\n","TRAIN_TEST_N_SPLITS = 3\n","\n","# Перезапуск всего обучения на полном train.csv и формирование/сохранение submission.csv\n","# на основе test.csv, необходимо для реального сабмита на каггле\n","GENERATE_SUBMISSION_CSV = True"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:43:20.590653Z","iopub.status.busy":"2022-04-29T06:43:20.590215Z","iopub.status.idle":"2022-04-29T06:43:20.593661Z","shell.execute_reply":"2022-04-29T06:43:20.593068Z","shell.execute_reply.started":"2022-04-29T06:43:20.590621Z"}},"source":["## Обработка датасетов"]},{"cell_type":"markdown","metadata":{},"source":["### - Разбиваем исходный Raw Train датасет (train.csv) на Train/Test (-> имеем на выходе split_train/split_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:43:27.937325Z","iopub.status.busy":"2022-05-02T09:43:27.936763Z","iopub.status.idle":"2022-05-02T09:43:45.855175Z","shell.execute_reply":"2022-05-02T09:43:45.854183Z","shell.execute_reply.started":"2022-05-02T09:43:27.937287Z"},"trusted":true},"outputs":[],"source":["# Разбиваем так, что бы объекты с одинаковым POI не раскидывались в разные выборки\n","\n","train = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, 'train.csv'))\n","\n","# Подход по применению GroupKFold для подобных целей подсмотрел здесь:\n","# https://www.kaggle.com/code/ryotayoshinobu/foursquare-lightgbm-baseline\n","\n","kf = GroupKFold(n_splits=TRAIN_TEST_N_SPLITS)\n","for i, (trn_idx, val_idx) in enumerate(kf.split(train, train['point_of_interest'], train['point_of_interest'])):\n","    train.loc[val_idx, \"parts\"] = str(i)\n","    \n","split_test = train[train.parts == '1'].drop(columns='parts')\n","split_train = train[~(train.parts == '1')].drop(columns='parts')\n","\n","print(f'Our train size: {len(split_train)}, Our test size: {len(split_test)}')\n","\n","#Освобождаем память\n","del train"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:27:54.356748Z","iopub.status.busy":"2022-04-29T06:27:54.356432Z","iopub.status.idle":"2022-04-29T06:27:54.360855Z","shell.execute_reply":"2022-04-29T06:27:54.360228Z","shell.execute_reply.started":"2022-04-29T06:27:54.356714Z"}},"source":["### - Предобработка датасета, отбор, формирование пар-кандидатов для сравнения (-> датасет пар-кандидатов)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:43:45.856649Z","iopub.status.busy":"2022-05-02T09:43:45.85642Z","iopub.status.idle":"2022-05-02T09:44:27.746997Z","shell.execute_reply":"2022-05-02T09:44:27.746112Z","shell.execute_reply.started":"2022-05-02T09:43:45.856622Z"},"trusted":true},"outputs":[],"source":["PAIRS_DROP_ORDER_DUBLICATES = True\n","\n","pairs_train, pairs_metrics = generate_pairs_df(split_train, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True)\n","print(pairs_metrics)\n","pairs_train"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:27:54.356748Z","iopub.status.busy":"2022-04-29T06:27:54.356432Z","iopub.status.idle":"2022-04-29T06:27:54.360855Z","shell.execute_reply":"2022-04-29T06:27:54.360228Z","shell.execute_reply.started":"2022-04-29T06:27:54.356714Z"}},"source":["### - Фиче инжинеринг датасета пар-кандидатов, формирование X, y (-> X, y, готовые для передачи в модель)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:44:27.748404Z","iopub.status.busy":"2022-05-02T09:44:27.748182Z","iopub.status.idle":"2022-05-02T09:46:33.990742Z","shell.execute_reply":"2022-05-02T09:46:33.98963Z","shell.execute_reply.started":"2022-05-02T09:44:27.748377Z"},"trusted":true},"outputs":[],"source":["## Формирование датасета, подходящего для передачи в модель\n","pairs_futures_pipeline = [\n","    {'func': add_feauture_geo_distance,\n","     'params': {}},\n","    {'func': add_feauture_levenshtein_distance,\n","     'params': {}}]\n","\n","#Генерируем все необходимые фичи\n","pairs_train, future_columns = run_futures_pipeline(pairs_train, pairs_futures_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:33.992371Z","iopub.status.busy":"2022-05-02T09:46:33.992112Z","iopub.status.idle":"2022-05-02T09:46:33.997693Z","shell.execute_reply":"2022-05-02T09:46:33.996647Z","shell.execute_reply.started":"2022-05-02T09:46:33.992341Z"},"trusted":true},"outputs":[],"source":["## Нормализация фичей (или нужно сразу при формировании фичей?)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:33.999416Z","iopub.status.busy":"2022-05-02T09:46:33.999197Z","iopub.status.idle":"2022-05-02T09:46:34.400651Z","shell.execute_reply":"2022-05-02T09:46:34.399385Z","shell.execute_reply.started":"2022-05-02T09:46:33.999389Z"},"trusted":true},"outputs":[],"source":["## Формируем X, y для дальнейшей передачи в модель\n","\n","y = get_match_label(pairs_train)\n","X = pairs_train[future_columns]\n","\n","# оставляем в памяти (для очистки лишней памяти) только те, колонки, \n","# которые нам понадобятся в дальнейшем (для формирования submission)\n","pairs_train = pairs_train[['id_1', 'id_2']]"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:58:44.051456Z","iopub.status.busy":"2022-04-29T06:58:44.051162Z","iopub.status.idle":"2022-04-29T06:58:44.055366Z","shell.execute_reply":"2022-04-29T06:58:44.054391Z","shell.execute_reply.started":"2022-04-29T06:58:44.051422Z"}},"source":["### - Обработка датасета отложенной выборки split_test для итоговой оценки модели (-> )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:34.404307Z","iopub.status.busy":"2022-05-02T09:46:34.403922Z","iopub.status.idle":"2022-05-02T09:47:32.646005Z","shell.execute_reply":"2022-05-02T09:47:32.645006Z","shell.execute_reply.started":"2022-05-02T09:46:34.404259Z"},"trusted":true},"outputs":[],"source":["pairs_test, pairs_metrics = generate_pairs_df(split_test, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True)\n","\n","print(pairs_metrics)\n","\n","pairs_test, future_columns = run_futures_pipeline(pairs_test, pairs_futures_pipeline)\n","\n","y_test = get_match_label(pairs_test)\n","X_test = pairs_test[future_columns]\n","\n","# оставляем в памяти (для очистки лишней памяти) только те, колонки, \n","# которые нам понадобятся в дальнейшем (для формирования submission)\n","pairs_test = pairs_test[['id_1', 'id_2']]"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:02:30.61852Z","iopub.status.busy":"2022-04-29T07:02:30.618258Z","iopub.status.idle":"2022-04-29T07:02:30.622406Z","shell.execute_reply":"2022-04-29T07:02:30.621479Z","shell.execute_reply.started":"2022-04-29T07:02:30.618493Z"}},"source":["## Обучение модели (-> сохраненная в файл модель)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:32.648169Z","iopub.status.busy":"2022-05-02T09:47:32.647613Z","iopub.status.idle":"2022-05-02T09:47:36.994574Z","shell.execute_reply":"2022-05-02T09:47:36.993829Z","shell.execute_reply.started":"2022-05-02T09:47:32.648119Z"},"trusted":true},"outputs":[],"source":["# Model params\n","model_params = {'random_state': 42,\n","                'n_estimators': 10,\n","                'verbosity': 0}\n","\n","# Define the model\n","model = XGBClassifier(**model_params)\n","\n","# Fit the model\n","model.fit(X, y)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:03:58.306023Z","iopub.status.busy":"2022-04-29T07:03:58.305628Z","iopub.status.idle":"2022-04-29T07:03:58.310055Z","shell.execute_reply":"2022-04-29T07:03:58.309275Z","shell.execute_reply.started":"2022-04-29T07:03:58.305988Z"}},"source":["## Оценка модели на отложенной выборке (-> Score на отложенной выборке)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:37.000805Z","iopub.status.busy":"2022-05-02T09:47:36.998336Z","iopub.status.idle":"2022-05-02T09:47:37.065976Z","shell.execute_reply":"2022-05-02T09:47:37.065276Z","shell.execute_reply.started":"2022-05-02T09:47:37.000759Z"},"trusted":true},"outputs":[],"source":["# Get predictions\n","y_pred = model.predict(X_test)\n","\n","# Accuracy (not all id's) удалить, т.к. не учитывает часть id\n","print(np.mean(y_pred == np.array(y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:37.072265Z","iopub.status.busy":"2022-05-02T09:47:37.069941Z","iopub.status.idle":"2022-05-02T09:47:41.509773Z","shell.execute_reply":"2022-05-02T09:47:41.508598Z","shell.execute_reply.started":"2022-05-02T09:47:37.072197Z"},"trusted":true},"outputs":[],"source":["submission_true = get_submission_true(split_test)\n","submission_pred = get_submission_predict(split_test, pairs_test, y_pred, PAIRS_DROP_ORDER_DUBLICATES)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:41.511587Z","iopub.status.busy":"2022-05-02T09:47:41.511262Z","iopub.status.idle":"2022-05-02T09:47:53.924868Z","shell.execute_reply":"2022-05-02T09:47:53.924054Z","shell.execute_reply.started":"2022-05-02T09:47:41.51155Z"},"trusted":true},"outputs":[],"source":["jaccard_score(submission_true, submission_pred)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:03:58.306023Z","iopub.status.busy":"2022-04-29T07:03:58.305628Z","iopub.status.idle":"2022-04-29T07:03:58.310055Z","shell.execute_reply":"2022-04-29T07:03:58.309275Z","shell.execute_reply.started":"2022-04-29T07:03:58.305988Z"}},"source":["## Перезапуск всего пайплайна на полном датасете для целей сабмита на каггле (-> submission.csv для сабмита на каггле)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:53.92643Z","iopub.status.busy":"2022-05-02T09:47:53.926181Z","iopub.status.idle":"2022-05-02T09:54:05.93671Z","shell.execute_reply":"2022-05-02T09:54:05.935396Z","shell.execute_reply.started":"2022-05-02T09:47:53.9264Z"},"trusted":true},"outputs":[],"source":["if GENERATE_SUBMISSION_CSV:\n","    train = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, 'train.csv'))\n","\n","    \n","    ## Отбираем кандидатов и формируем парный датасет\n","    pairs_train, pairs_metrics = generate_pairs_df(train, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True)\n","    print(pairs_metrics)\n","\n","    \n","    ## Генерируем все необходимые фичи\n","    pairs_train, future_columns = run_futures_pipeline(pairs_train, pairs_futures_pipeline)\n","    \n","    \n","    ## Формируем X, y для дальнейшей передачи в модель\n","    y = get_match_label(pairs_train)\n","    X = pairs_train[future_columns]\n","\n","    # оставляем в памяти (для очистки лишней памяти) только те, колонки, \n","    # которые нам понадобятся в дальнейшем (для формирования submission)\n","    pairs_train = pairs_train[['id_1', 'id_2']]\n","    \n","    \n","    ## Обучаем модель\n","    \n","    # Model params\n","    model_params = {'random_state': 42,\n","                    'n_estimators': 10,\n","                    'verbosity': 0}\n","\n","    # Define the model\n","    model = XGBClassifier(**model_params)\n","\n","    # Fit the model\n","    model.fit(X, y)\n","    \n","    \n","    ## Генерируем предсказания и финальный submission.csv\n","    test = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, 'test.csv'))\n","    \n","    # Т.к. публичный (в отличе от приветного) test.csv содержит всех 5 разрозненных записей, которые даже не\n","    # попадают в кандидаты, весь пайплайн рушится, по причине пустого парного датасета,\n","    # что бы это обойти, проверяем на количество строк (с целью определить имеем мы дело с публичным или \n","    # приватным test.csv) и в случае публичного, дублируем объекты изменив предварительно id\n","    if len(test) == 5:\n","        temp = test.copy()\n","        temp.id = temp.id + '_'\n","        test = pd.concat([test, temp])\n","    \n","    pairs_test, _ = generate_pairs_df(test, PAIRS_DROP_ORDER_DUBLICATES, real_test=True)\n","\n","    pairs_test, future_columns = run_futures_pipeline(pairs_test, pairs_futures_pipeline)\n","\n","    X_test = pairs_test[future_columns]\n","\n","    # оставляем в памяти (для очистки лишней памяти) только те, колонки, \n","    # которые нам понадобятся в дальнейшем (для формирования submission)\n","    pairs_test = pairs_test[['id_1', 'id_2']]\n","    \n","    # Get predictions\n","    y_pred = model.predict(X_test)\n","\n","    submission_pred = get_submission_predict(test, pairs_test, y_pred, PAIRS_DROP_ORDER_DUBLICATES)\n","    \n","    submission_pred.to_csv(\"submission.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
