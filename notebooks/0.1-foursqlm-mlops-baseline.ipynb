{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-04T05:20:55.705883Z","iopub.status.busy":"2022-05-04T05:20:55.705293Z","iopub.status.idle":"2022-05-04T05:20:55.710929Z","shell.execute_reply":"2022-05-04T05:20:55.709947Z","shell.execute_reply.started":"2022-05-04T05:20:55.705844Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"]},{"cell_type":"markdown","metadata":{},"source":["## Префиксный код (импорты, вспомогательные функции/классы/гиперпараметры)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:40.238733Z","iopub.status.busy":"2022-05-02T16:32:40.238475Z","iopub.status.idle":"2022-05-02T16:32:41.539092Z","shell.execute_reply":"2022-05-02T16:32:41.538405Z","shell.execute_reply.started":"2022-05-02T16:32:40.238693Z"},"trusted":true},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import haversine as hs\n","import random\n","\n","# Imports from\n","from haversine import Unit\n","from sklearn.model_selection import GroupKFold\n","from fuzzywuzzy import fuzz\n","from xgboost import XGBClassifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.541334Z","iopub.status.busy":"2022-05-02T16:32:41.540807Z","iopub.status.idle":"2022-05-02T16:32:41.546388Z","shell.execute_reply":"2022-05-02T16:32:41.54508Z","shell.execute_reply.started":"2022-05-02T16:32:41.541285Z"},"trusted":true},"outputs":[],"source":["# Fix all random seeds\n","random.seed(42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.548257Z","iopub.status.busy":"2022-05-02T16:32:41.54785Z","iopub.status.idle":"2022-05-02T16:32:41.581993Z","shell.execute_reply":"2022-05-02T16:32:41.581393Z","shell.execute_reply.started":"2022-05-02T16:32:41.548221Z"},"trusted":true},"outputs":[],"source":["# Custom Func's/Class's\n","\n","\n","def jaccard(list_a: list, list_b: list) -> float:\n","    a = set(list_a)\n","    b = set(list_b)\n","    return len(a & b) / len(a | b)\n","\n","\n","def jaccard_score(df_sub_a: pd.DataFrame, df_sub_b: pd.DataFrame) -> float:\n","\n","    assert len(df_sub_a) == len(df_sub_b)\n","    df = pd.merge(df_sub_a, df_sub_b, on=\"id\", suffixes=[\"_a\", \"_b\"])\n","    return (df.apply(lambda x: jaccard(x.matches_a.split(), x.matches_b.split()), axis=1)).mean()\n","\n","\n","def get_submission_predict(\n","    df_original: pd.DataFrame,\n","    df_pairs: pd.DataFrame,\n","    labels: np.array,\n","    pairs_drop_orders_dublicates=False,\n",") -> pd.DataFrame:\n","\n","    # task: понял поздно, уже после написания, но есть проблема, нужно ее решить,\n","    #       переписать в единственном варианте, либо добавить возможность возвращать\n","    #       оба варианта, в зависимости от перданного параметра (и проверить на\n","    #       реальных сабмитах, какой вариант дает больший скор);\n","    #       проблема: если модель предсказала, что (id1, id2) и (id2, id3) дублиаты,\n","    #       а (id1, id3) нет, то в текущей реализации в сабмит добавятся строки\n","    #       (id1, [id1, id2]), (id2, [id2, id1, id3], (id3, [id3, id2]), хотя на\n","    #       самом деле они либо все дубликаты, либо где то предсказание ошибочно.\n","\n","    # task: возможно медленно работает, протестировать с прошлой версией во времени\n","\n","    # формируем датасет из пар, для которых match/label == 1\n","    df_pairs = df_pairs[[\"id_1\", \"id_2\"]]\n","    df_pairs[\"match\"] = labels\n","    df_pairs = df_pairs[df_pairs.match == 1][[\"id_1\", \"id_2\"]]\n","\n","    # если мы оставляли пары только в одном направлении (id_1, id_2),\n","    # то возвращаем что бы они были в обоих (id_1, id_2) и (id_2, id_1)\n","    if pairs_drop_orders_dublicates:\n","        df_pairs = pd.concat(\n","            [df_pairs, df_pairs.rename(columns={\"id_1\": \"id_2\", \"id_2\": \"id_1\"})]\n","        ).drop_duplicates()  # drop_duplicates не обязателен\n","\n","    # добавляем сапоставление  id  самому себе, т.к. этого требует выходной\n","    # формат\n","    pairs_one_to_one = pd.DataFrame({\"id_1\": df_pairs.id_1.unique()})\n","    pairs_one_to_one[\"id_2\"] = pairs_one_to_one.id_1\n","    df_pairs = pd.concat([pairs_one_to_one, df_pairs])\n","\n","    # переводим в формат id, matches, где в matches через пробел перечислены все\n","    # найденные дубликаты (в том числе сам id попадет в matches)\n","    df_pairs = (\n","        df_pairs.groupby(\"id_1\")\n","        .id_2.agg(\" \".join)\n","        .to_frame()\n","        .reset_index()\n","        .rename(columns={\"id_1\": \"id\", \"id_2\": \"matches\"})\n","    )\n","\n","    # в df_pairs остались только id, для которых найдены дубликаты, мерджим со\n","    # всеми id из исходного датасета и добавляем в matchs id самого себя, для\n","    # тех id, которые не попали в df_pairs (после merge у них matches == NaN)\n","    df_submission = pd.merge(df_original[\"id\"], df_pairs, on=\"id\", how=\"left\")\n","    df_submission[\"matches\"] = df_submission.matches.fillna(df_submission.id)\n","\n","    assert len(df_submission) == len(df_original)\n","\n","    return df_submission\n","\n","\n","def get_submission_true(df_original: pd.DataFrame) -> pd.DataFrame:\n","    df_original = df_original[[\"id\", \"point_of_interest\"]]\n","    df_poi_matches = (\n","        df_original.groupby(\"point_of_interest\")\n","        .id.agg(\" \".join)\n","        .to_frame()\n","        .reset_index()\n","        .rename(columns={\"id\": \"matches\"})\n","    )\n","    return pd.merge(df_original, df_poi_matches, on=\"point_of_interest\", how=\"left\")[\n","        [\"id\", \"matches\"]\n","    ]\n","\n","\n","def get_pairs_metrics(\n","    df_original: pd.DataFrame,\n","    df_pairs: pd.DataFrame,\n","    labels: np.array,\n","    pairs_drop_orders_dublicates=False,\n",") -> dict:\n","    metrics = {}\n","    submission_true = get_submission_true(df_original)\n","    submission_pairs_max_true = get_submission_predict(\n","        df_original, df_pairs, labels, pairs_drop_orders_dublicates\n","    )\n","    metrics[\"Jaccard (max)\"] = jaccard_score(submission_true, submission_pairs_max_true)\n","\n","    return metrics\n","\n","\n","def generate_pairs_df(\n","    df_dataset: pd.DataFrame, drop_order_dub=False, get_metrics=False, real_test=False\n",") -> (pd.DataFrame, dict):\n","    # Отбираем только колонки, которые планируем использовать в дальнейшем\n","    # task: перенести в параметры/гиперпараметры?\n","\n","    FIRST_COLUMN_SELECTION = [\n","        \"id\",\n","        \"name\",\n","        \"latitude\",\n","        \"longitude\",\n","        \"country\",\n","        \"city\",\n","        \"categories\",\n","        \"point_of_interest\",\n","    ]\n","\n","    # в реальном тесте отсутсвует 'point_of_interest', удаляем из наших колонок\n","    if real_test:\n","        FIRST_COLUMN_SELECTION.remove(\"point_of_interest\")\n","\n","    df_dataset = df_dataset[FIRST_COLUMN_SELECTION]\n","\n","    # task: Изучить возможности sklearn для подобных целей, скорей всего это будет наилучший вариант (примерные ключевые слова:\n","    # sklearn neighbors by coordinate)\n","    FIRST_COORD_ROUND = 3  # сотые ~= 1 км, тысячные ~= 0.1 км\n","\n","    # task: Если использовать подобный подход, нужно обязательно производить с перекрытием (придумать как)\n","    # task: Устранить SettingWithCopyWarning\n","\n","    # Первоначальный вариант (в 'lat_lon_round' мы получим строкивые представления округленных координат):\n","    # df_dataset.loc['lat_lon_group'] = (df_dataset.latitude.map(lambda x: str(round(x,FIRST_COORD_ROUND))) + '_' +\n","    #                                    df_dataset.longitude.map(lambda x: str(round(x,FIRST_COORD_ROUND))))\n","    # Альтернативный вариант (результат аналогичный, за исключенем того, что в 'lat_lon_round' мы получим номера групп):\n","    df_dataset[\"lat_lon_group\"] = df_dataset.groupby(\n","        [\n","            df_dataset.latitude.round(FIRST_COORD_ROUND),\n","            df_dataset.longitude.round(FIRST_COORD_ROUND),\n","        ]\n","    ).ngroup()\n","\n","    ## Формирование пар-кандидитов\n","    columns_to_pairs = [\"lat_lon_group\"]  # колонки для совоставления в пары\n","    df_pairs = pd.merge(df_dataset, df_dataset, on=columns_to_pairs, suffixes=[\"_1\", \"_2\"])\n","\n","    # Оставляем пары только в одном направлении (id_1, id_2) или в обоих (id_1, id_2) и (id_2, id_1)\n","    if drop_order_dub:\n","        df_pairs = df_pairs[df_pairs.id_1 < df_pairs.id_2]\n","    else:  # удаляем только полные дубликаты (id_1, id_1)\n","        df_pairs = df_pairs[df_pairs.id_1 != df_pairs.id_2]\n","\n","    # Generate metrics for current candidates\n","    metrics = {}\n","    if get_metrics and not real_test:\n","        labels = np.array(get_match_label(df_pairs))\n","        metrics = get_pairs_metrics(df_dataset, df_pairs, labels, drop_order_dub)\n","\n","    return df_pairs, metrics\n","\n","\n","def add_feauture_geo_distance(\n","    df_pairs: pd.DataFrame, normalize=False, prefix=\"ftr_\"\n",") -> pd.DataFrame:\n","    # Считаем расстояние в км между точками\n","    # task: Возможно нет смысла считать точно через haversine (с учетом шарообразности земли),\n","    # можно считать более грубо, но быстрее\n","\n","    df_pairs[f\"{prefix}geo_distance\"] = df_pairs.apply(\n","        lambda x: hs.haversine(\n","            (x.latitude_1, x.longitude_1),\n","            (x.latitude_2, x.longitude_2),\n","            unit=Unit.KILOMETERS,\n","        ),\n","        axis=1,\n","    )\n","    return df_pairs\n","\n","\n","def add_feauture_levenshtein_distance(\n","    df_pairs: pd.DataFrame, normalize=False, prefix=\"ftr_\"\n",") -> pd.DataFrame:\n","    # Levenshtein distance of names\n","    df_pairs[f\"{prefix}name_levenshtein\"] = df_pairs.apply(\n","        lambda x: fuzz.token_set_ratio(x.name_1, x.name_2), axis=1\n","    )\n","    return df_pairs\n","\n","\n","def run_futures_pipeline(\n","    dataset: pd.DataFrame, futures_pipeline: list, prefix=\"ftr_\"\n",") -> pd.DataFrame:\n","    for step in futures_pipeline:\n","        dataset = step[\"func\"](dataset, prefix=prefix, **step[\"params\"])\n","    future_columns = [col for col in dataset.columns if col.startswith(prefix)]\n","    return dataset.reset_index(drop=True), future_columns\n","\n","\n","def get_match_label(dataset: pd.DataFrame) -> pd.Series:  # 1 = match, 0 = not match\n","    # Наша целевая переменная (label/target)\n","    return (dataset[\"point_of_interest_1\"] == dataset[\"point_of_interest_2\"]).astype(int)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T16:32:41.583753Z","iopub.status.busy":"2022-05-02T16:32:41.583513Z","iopub.status.idle":"2022-05-02T16:32:41.601279Z","shell.execute_reply":"2022-05-02T16:32:41.600404Z","shell.execute_reply.started":"2022-05-02T16:32:41.583726Z"},"trusted":true},"outputs":[],"source":["## Hyperparameters\n","\n","# Задаем пути до директории с train/test.csv (в записимости от варианта запуска ноутбука)\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n","    PATH_TO_RAW_DATA = \"/kaggle/input/foursquare-location-matching\"\n","else:\n","    # ./data/raw\n","    PATH_TO_RAW_DATA = os.path.join(\".\", \"data\", \"raw\")\n","\n","# Количество частей, на которые будет разбит train датасет,\n","# одна часть уйдет на split_test, остальные на split_train\n","# 3 = 1/2 (или 33%/88%), 4 = 1/3 (или 25%/75) и т.д.\n","TRAIN_TEST_N_SPLITS = 3\n","\n","# Перезапуск всего обучения на полном train.csv и формирование/сохранение submission.csv\n","# на основе test.csv, необходимо для реального сабмита на каггле\n","GENERATE_SUBMISSION_CSV = True\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:43:20.590653Z","iopub.status.busy":"2022-04-29T06:43:20.590215Z","iopub.status.idle":"2022-04-29T06:43:20.593661Z","shell.execute_reply":"2022-04-29T06:43:20.593068Z","shell.execute_reply.started":"2022-04-29T06:43:20.590621Z"}},"source":["## Обработка датасетов"]},{"cell_type":"markdown","metadata":{},"source":["### - Разбиваем исходный Raw Train датасет (train.csv) на Train/Test (-> имеем на выходе split_train/split_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:43:27.937325Z","iopub.status.busy":"2022-05-02T09:43:27.936763Z","iopub.status.idle":"2022-05-02T09:43:45.855175Z","shell.execute_reply":"2022-05-02T09:43:45.854183Z","shell.execute_reply.started":"2022-05-02T09:43:27.937287Z"},"trusted":true},"outputs":[],"source":["# Разбиваем так, что бы объекты с одинаковым POI не раскидывались в разные выборки\n","\n","train = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, \"train.csv\"))\n","\n","# Подход по применению GroupKFold для подобных целей подсмотрел здесь:\n","# https://www.kaggle.com/code/ryotayoshinobu/foursquare-lightgbm-baseline\n","\n","kf = GroupKFold(n_splits=TRAIN_TEST_N_SPLITS)\n","for i, (trn_idx, val_idx) in enumerate(\n","    kf.split(train, train[\"point_of_interest\"], train[\"point_of_interest\"])\n","):\n","    train.loc[val_idx, \"parts\"] = str(i)\n","\n","split_test = train[train.parts == \"1\"].drop(columns=\"parts\")\n","split_train = train[~(train.parts == \"1\")].drop(columns=\"parts\")\n","\n","print(f\"Our train size: {len(split_train)}, Our test size: {len(split_test)}\")\n","\n","# Освобождаем память\n","del train\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:27:54.356748Z","iopub.status.busy":"2022-04-29T06:27:54.356432Z","iopub.status.idle":"2022-04-29T06:27:54.360855Z","shell.execute_reply":"2022-04-29T06:27:54.360228Z","shell.execute_reply.started":"2022-04-29T06:27:54.356714Z"}},"source":["### - Предобработка датасета, отбор, формирование пар-кандидатов для сравнения (-> датасет пар-кандидатов)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:43:45.856649Z","iopub.status.busy":"2022-05-02T09:43:45.85642Z","iopub.status.idle":"2022-05-02T09:44:27.746997Z","shell.execute_reply":"2022-05-02T09:44:27.746112Z","shell.execute_reply.started":"2022-05-02T09:43:45.856622Z"},"trusted":true},"outputs":[],"source":["PAIRS_DROP_ORDER_DUBLICATES = True\n","\n","pairs_train, pairs_metrics = generate_pairs_df(\n","    split_train, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True\n",")\n","print(pairs_metrics)\n","pairs_train\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:27:54.356748Z","iopub.status.busy":"2022-04-29T06:27:54.356432Z","iopub.status.idle":"2022-04-29T06:27:54.360855Z","shell.execute_reply":"2022-04-29T06:27:54.360228Z","shell.execute_reply.started":"2022-04-29T06:27:54.356714Z"}},"source":["### - Фиче инжинеринг датасета пар-кандидатов, формирование X, y (-> X, y, готовые для передачи в модель)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:44:27.748404Z","iopub.status.busy":"2022-05-02T09:44:27.748182Z","iopub.status.idle":"2022-05-02T09:46:33.990742Z","shell.execute_reply":"2022-05-02T09:46:33.98963Z","shell.execute_reply.started":"2022-05-02T09:44:27.748377Z"},"trusted":true},"outputs":[],"source":["## Формирование датасета, подходящего для передачи в модель\n","pairs_futures_pipeline = [\n","    {\"func\": add_feauture_geo_distance, \"params\": {}},\n","    {\"func\": add_feauture_levenshtein_distance, \"params\": {}},\n","]\n","\n","# Генерируем все необходимые фичи\n","pairs_train, future_columns = run_futures_pipeline(pairs_train, pairs_futures_pipeline)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:33.992371Z","iopub.status.busy":"2022-05-02T09:46:33.992112Z","iopub.status.idle":"2022-05-02T09:46:33.997693Z","shell.execute_reply":"2022-05-02T09:46:33.996647Z","shell.execute_reply.started":"2022-05-02T09:46:33.992341Z"},"trusted":true},"outputs":[],"source":["## Нормализация фичей (или нужно сразу при формировании фичей?)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:33.999416Z","iopub.status.busy":"2022-05-02T09:46:33.999197Z","iopub.status.idle":"2022-05-02T09:46:34.400651Z","shell.execute_reply":"2022-05-02T09:46:34.399385Z","shell.execute_reply.started":"2022-05-02T09:46:33.999389Z"},"trusted":true},"outputs":[],"source":["## Формируем X, y для дальнейшей передачи в модель\n","\n","y = get_match_label(pairs_train)\n","X = pairs_train[future_columns]\n","\n","# оставляем в памяти (для очистки лишней памяти) только те, колонки,\n","# которые нам понадобятся в дальнейшем (для формирования submission)\n","pairs_train = pairs_train[[\"id_1\", \"id_2\"]]\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T06:58:44.051456Z","iopub.status.busy":"2022-04-29T06:58:44.051162Z","iopub.status.idle":"2022-04-29T06:58:44.055366Z","shell.execute_reply":"2022-04-29T06:58:44.054391Z","shell.execute_reply.started":"2022-04-29T06:58:44.051422Z"}},"source":["### - Обработка датасета отложенной выборки split_test для итоговой оценки модели (-> )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:46:34.404307Z","iopub.status.busy":"2022-05-02T09:46:34.403922Z","iopub.status.idle":"2022-05-02T09:47:32.646005Z","shell.execute_reply":"2022-05-02T09:47:32.645006Z","shell.execute_reply.started":"2022-05-02T09:46:34.404259Z"},"trusted":true},"outputs":[],"source":["pairs_test, pairs_metrics = generate_pairs_df(\n","    split_test, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True\n",")\n","\n","print(pairs_metrics)\n","\n","pairs_test, future_columns = run_futures_pipeline(pairs_test, pairs_futures_pipeline)\n","\n","y_test = get_match_label(pairs_test)\n","X_test = pairs_test[future_columns]\n","\n","# оставляем в памяти (для очистки лишней памяти) только те, колонки,\n","# которые нам понадобятся в дальнейшем (для формирования submission)\n","pairs_test = pairs_test[[\"id_1\", \"id_2\"]]\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:02:30.61852Z","iopub.status.busy":"2022-04-29T07:02:30.618258Z","iopub.status.idle":"2022-04-29T07:02:30.622406Z","shell.execute_reply":"2022-04-29T07:02:30.621479Z","shell.execute_reply.started":"2022-04-29T07:02:30.618493Z"}},"source":["## Обучение модели (-> сохраненная в файл модель)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:32.648169Z","iopub.status.busy":"2022-05-02T09:47:32.647613Z","iopub.status.idle":"2022-05-02T09:47:36.994574Z","shell.execute_reply":"2022-05-02T09:47:36.993829Z","shell.execute_reply.started":"2022-05-02T09:47:32.648119Z"},"trusted":true},"outputs":[],"source":["# Model params\n","model_params = {\"random_state\": 42, \"n_estimators\": 10, \"verbosity\": 0}\n","\n","# Define the model\n","model = XGBClassifier(**model_params)\n","\n","# Fit the model\n","model.fit(X, y)\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:03:58.306023Z","iopub.status.busy":"2022-04-29T07:03:58.305628Z","iopub.status.idle":"2022-04-29T07:03:58.310055Z","shell.execute_reply":"2022-04-29T07:03:58.309275Z","shell.execute_reply.started":"2022-04-29T07:03:58.305988Z"}},"source":["## Оценка модели на отложенной выборке (-> Score на отложенной выборке)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:37.000805Z","iopub.status.busy":"2022-05-02T09:47:36.998336Z","iopub.status.idle":"2022-05-02T09:47:37.065976Z","shell.execute_reply":"2022-05-02T09:47:37.065276Z","shell.execute_reply.started":"2022-05-02T09:47:37.000759Z"},"trusted":true},"outputs":[],"source":["# Get predictions\n","y_pred = model.predict(X_test)\n","\n","# Accuracy (not all id's) удалить, т.к. не учитывает часть id\n","print(np.mean(y_pred == np.array(y_test)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:37.072265Z","iopub.status.busy":"2022-05-02T09:47:37.069941Z","iopub.status.idle":"2022-05-02T09:47:41.509773Z","shell.execute_reply":"2022-05-02T09:47:41.508598Z","shell.execute_reply.started":"2022-05-02T09:47:37.072197Z"},"trusted":true},"outputs":[],"source":["submission_true = get_submission_true(split_test)\n","submission_pred = get_submission_predict(\n","    split_test, pairs_test, y_pred, PAIRS_DROP_ORDER_DUBLICATES\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:41.511587Z","iopub.status.busy":"2022-05-02T09:47:41.511262Z","iopub.status.idle":"2022-05-02T09:47:53.924868Z","shell.execute_reply":"2022-05-02T09:47:53.924054Z","shell.execute_reply.started":"2022-05-02T09:47:41.51155Z"},"trusted":true},"outputs":[],"source":["jaccard_score(submission_true, submission_pred)\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-29T07:03:58.306023Z","iopub.status.busy":"2022-04-29T07:03:58.305628Z","iopub.status.idle":"2022-04-29T07:03:58.310055Z","shell.execute_reply":"2022-04-29T07:03:58.309275Z","shell.execute_reply.started":"2022-04-29T07:03:58.305988Z"}},"source":["## Перезапуск всего пайплайна на полном датасете для целей сабмита на каггле (-> submission.csv для сабмита на каггле)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T09:47:53.92643Z","iopub.status.busy":"2022-05-02T09:47:53.926181Z","iopub.status.idle":"2022-05-02T09:54:05.93671Z","shell.execute_reply":"2022-05-02T09:54:05.935396Z","shell.execute_reply.started":"2022-05-02T09:47:53.9264Z"},"trusted":true},"outputs":[],"source":["if GENERATE_SUBMISSION_CSV:\n","    train = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, \"train.csv\"))\n","\n","    ## Отбираем кандидатов и формируем парный датасет\n","    pairs_train, pairs_metrics = generate_pairs_df(\n","        train, PAIRS_DROP_ORDER_DUBLICATES, get_metrics=True\n","    )\n","    print(pairs_metrics)\n","\n","    ## Генерируем все необходимые фичи\n","    pairs_train, future_columns = run_futures_pipeline(pairs_train, pairs_futures_pipeline)\n","\n","    ## Формируем X, y для дальнейшей передачи в модель\n","    y = get_match_label(pairs_train)\n","    X = pairs_train[future_columns]\n","\n","    # оставляем в памяти (для очистки лишней памяти) только те, колонки,\n","    # которые нам понадобятся в дальнейшем (для формирования submission)\n","    pairs_train = pairs_train[[\"id_1\", \"id_2\"]]\n","\n","    ## Обучаем модель\n","\n","    # Model params\n","    model_params = {\"random_state\": 42, \"n_estimators\": 10, \"verbosity\": 0}\n","\n","    # Define the model\n","    model = XGBClassifier(**model_params)\n","\n","    # Fit the model\n","    model.fit(X, y)\n","\n","    ## Генерируем предсказания и финальный submission.csv\n","    test = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, \"test.csv\"))\n","\n","    # Т.к. публичный (в отличе от приветного) test.csv содержит всех 5 разрозненных записей, которые даже не\n","    # попадают в кандидаты, весь пайплайн рушится, по причине пустого парного датасета,\n","    # что бы это обойти, проверяем на количество строк (с целью определить имеем мы дело с публичным или\n","    # приватным test.csv) и в случае публичного, дублируем объекты изменив предварительно id\n","    if len(test) == 5:\n","        temp = test.copy()\n","        temp.id = temp.id + \"_\"\n","        test = pd.concat([test, temp])\n","\n","    pairs_test, _ = generate_pairs_df(test, PAIRS_DROP_ORDER_DUBLICATES, real_test=True)\n","\n","    pairs_test, future_columns = run_futures_pipeline(pairs_test, pairs_futures_pipeline)\n","\n","    X_test = pairs_test[future_columns]\n","\n","    # оставляем в памяти (для очистки лишней памяти) только те, колонки,\n","    # которые нам понадобятся в дальнейшем (для формирования submission)\n","    pairs_test = pairs_test[[\"id_1\", \"id_2\"]]\n","\n","    # Get predictions\n","    y_pred = model.predict(X_test)\n","\n","    submission_pred = get_submission_predict(test, pairs_test, y_pred, PAIRS_DROP_ORDER_DUBLICATES)\n","\n","    submission_pred.to_csv(\"submission.csv\", index=False)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
